{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fsJTIQ5Z770c"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torch.utils.data as data\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.datasets as datasets\n",
        "import torchvision.models as models\n",
        "import os\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "from collections import namedtuple"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "koI-Wa3K7hdG"
      },
      "outputs": [],
      "source": [
        "# ResNet은 여러 개의 Block으로 구성되어 있다\n",
        "# 이미지들이 Block을 통과하는데, 해당 Block을 통과할 때마다 Block의 output과 input이 그대로 더해진다\n",
        "# output과 input의 차원이 다르면 downsample과정을 통하여 input의 차원을 output의 차원에 맞도록 조정하는데\n",
        "# 이러한 과정을 skip connection이라고 한다\n",
        "\n",
        "class BasicBlock(nn.Module):\n",
        "  expansion = 1\n",
        "\n",
        "  def __init__(self, in_channels, out_channels, stride = 1, downsample = False):\n",
        "    super().__init__()\n",
        "    self.conv1 = nn.Conv2D(in_channels, out_channels, kernel_size = 3, stride = stride, padding=1, bias = False)\n",
        "    self.bn1 = nn.BatchNorm2d(out_channels)\n",
        "    self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "    self.bn2 = nn.BatchNorm2d(out_channels)\n",
        "    self.relu = nn.ReLU(inplace = True)\n",
        "\n",
        "    if downsample:\n",
        "      conv = nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride, bias=False)\n",
        "      bn = nn.BatchNorm2d(out_channels)\n",
        "      downsample = nn.Sequential(conv.bn)\n",
        "    else:\n",
        "      downsample = None\n",
        "    self.downsample = downsample\n",
        "\n",
        "  def forward(self, x):\n",
        "    i = x\n",
        "    x = self.conv1(x)\n",
        "    x = self.bn1(x)\n",
        "    x = self.relu(x)\n",
        "    x = self.conv2(x)\n",
        "    x = self.bn2(x)\n",
        "\n",
        "    if self.downsample is not None:\n",
        "      i = self.downsample(i)\n",
        "\n",
        "    x += i\n",
        "    x = self.relu(x)\n",
        "\n",
        "    return x\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8dGyS5uX9vpV"
      },
      "outputs": [],
      "source": [
        "# ResNet50, 101, 152에서는 기본 Block 이외에 BottleNeck Block이라는 특수한 구조의 Block을 사용한다\n",
        "# Block과 BottleNeck은 input을 output에 더해준다는 면에서 똑같다고 볼 수 있다.\n",
        "# 똑같은 input과 똑같은 output형상이 나온다고 해도 BottleNeck을 특수한 구조로 설계하여\n",
        "# 훨씬 더 Weight가 적게 할 수 있다.\n",
        "# 그로 인해 Model의 Complexity가 감소하게 된다\n",
        "\n",
        "class Bottleneck(nn.Module):\n",
        "  expansion = 4\n",
        "  # resnet 모델 에서 BottleNeck block은 차례대로 64 channels, 64 channel, 256 channels인 conv계층으로 이루어져있다.\n",
        "  # 해당 64channels에서 256channels로 맞추기 위하여 expansion값(4)를 곱해주는 것이다\n",
        "\n",
        "  def __init__(self, in_channels, out_channels, stride=1, downsample=False):\n",
        "    super().__init__()\n",
        "    self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=1, bias=False)\n",
        "    self.bn1 = nn.BatchNorm2d(out_channels)\n",
        "    self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False)\n",
        "    self.bn2 = nn.BatchNorm2d(out_channels)\n",
        "    self.conv3 = nn.Conv2d(out_channels, self.expansion * out_channels, kernel_size=1, stride=1, bias=False)\n",
        "    self.bn3 = nn.BatchNorm2d(self.expansion * out_channels)\n",
        "    self.relu = nn.ReLU(inplace=True)\n",
        "\n",
        "    if downsample:\n",
        "      conv = nn.Conv2d(in_channels, self.expansion * out_channels, kernel_size=1, stride=stride, bias=False)\n",
        "      bn = nn.BatchNorm2d(self.expansion * out_channels)\n",
        "      downsample = nn.Sequential(conv, bn)\n",
        "    else:\n",
        "      downsample = None\n",
        "    self.downsample = downsample\n",
        "\n",
        "  def forward(self, x):\n",
        "    i = x\n",
        "    x = self.conv1(x)\n",
        "    x = self.bn1(x)\n",
        "    x = self.relu(x)\n",
        "    x = self.conv2(x)\n",
        "    x = self.bn2(x)\n",
        "    x = self.relu(x)\n",
        "    x = self.conv3(x)\n",
        "    x = self.bn3(x)\n",
        "\n",
        "    if self.downsample is not None:\n",
        "      i = self.downsample(i)\n",
        "\n",
        "    x += i\n",
        "    x = self.relu(x)\n",
        "    return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kTtQUROMBPek"
      },
      "outputs": [],
      "source": [
        "# ResNet모델에 대한 네트워크 정의하기\n",
        "class ResNet(nn.Module):\n",
        "  def __init__(self, config, output_dim, zero_init_residual=False):\n",
        "    super().__init__()\n",
        "\n",
        "    # ResNet을 호출할 때 넘겨준 config값에 따라서 다양한 ResNet 모델을 만들 수 있도록 정의한 것이다\n",
        "    # 즉 config기반의 Model을 만들기 위해서 config라는 변수에 설정 parameter를 넣을 수 있도록 한 것이다\n",
        "    block, n_blocks, channels = config\n",
        "    self.in_channels = channels[0]\n",
        "    assert len(n_blocks) == len(channels) == 4\n",
        "\n",
        "    self.conv1 = nn.Conv2d(3, self.in_channels, kernel_size=7, stride=2, padding=3, bias=False)\n",
        "    self.bn1 = nn.BatchNorm2d(self.in_channels)\n",
        "    self.relu = nn.ReLU(inplace = True)\n",
        "    self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
        "\n",
        "    self.layer1 = self.get_resnet_layer(block, n_blocks[0], channels[0])\n",
        "    self.layer2 = self.get_resnet_layer(block, n_blocks[1], channels[1], stride=2)\n",
        "    self.layer3 = self.get_resnet_layer(block, n_blocks[2], channels[2], stride=2)\n",
        "    self.layer4 = self.get_resnet_layer(block, n_blocks[3], channels[3], stride=2)\n",
        "\n",
        "    self.avgpool = nn.AdaptiveAvgPool2d((1,1))\n",
        "    self.fc = nn.Linear(self.in_channels, output_dim)\n",
        "\n",
        "\n",
        "    # 각 Residual Block의 마지막 Batch normalization을 0으로 초기화하여 다음 Residual 분기를 0에서 시작할 수 있도록 한다\n",
        "    # 논문에 의하면 이러한 방식을 사용할 경우 모델 성능이 0.2 ~ 0.3%정도 향상한다고 하여 많은 ResNet에서 이용하고 있다\n",
        "    if zero_init_residual:\n",
        "      for m in self.modules():\n",
        "        if isinstance(m, Bottleneck):\n",
        "          nn.init.constant_(m.bn3.weight, 0)\n",
        "        elif isinstance(m ,BasicBlock):\n",
        "          nn.init.constant_(m.bn2.weight, 0)\n",
        "\n",
        "  def get_resnet_layer(self, block, n_blocks, channels, stride=1):\n",
        "    layers = []\n",
        "\n",
        "    # 만약 해당 Block에서 입력채널과 출력채널이 다르다면 Downsampling을 해주어야 한다\n",
        "    if self.in_channels != block.expansion * channels:\n",
        "      downsample = True\n",
        "    else:\n",
        "      downsample = False\n",
        "\n",
        "    layers.append(block(self.in_channels, channels, stride, downsample))\n",
        "    for i in range(1, n_blocks):\n",
        "      layers.append(block(block.expansion * channels, channels))\n",
        "\n",
        "    self.in_channels = block.expansion * channels\n",
        "    return nn.Sequential(*layers)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.conv1(x)\n",
        "    x = self.bn1(x)\n",
        "    x = self.relu(x)\n",
        "    x = self.maxpool(x)\n",
        "    x = self.layer1(x)\n",
        "    x = self.layer2(x)\n",
        "    x = self.layer3(x)\n",
        "    x = self.layer4(x)\n",
        "    x = self.avgpool(x)\n",
        "    x = torch.flatten(x,1) # Batch 단위로 Flatten하여 Dense layer에 입력될 수 있도록 한다\n",
        "    x = self.fc(x)\n",
        "    return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ircom6Q5Eo3G"
      },
      "outputs": [],
      "source": [
        "# ResNet모델을 만들 때 config옵션을 전달하여 다양하고 가지각색의 ResNet모델을 만들 수 있도록 정의하였다.\n",
        "# 다음과 같이 namedTuple형태로 config를 정의할 수 있도록 하였다.\n",
        "\n",
        "ResNetConfig = namedtuple('ResNetConfig',['block','n_blocks', 'channels'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EOaMJOGqFrpU"
      },
      "outputs": [],
      "source": [
        "# Resnet18, 34 model에는 BottleNeck block이 아닌 BasicBlock을 사용한다.\n",
        "# block : 모델에서 사용하는 블록의 종류 (BasicBlock이냐 BottleNeck Block이냐)\n",
        "# n_blocks : 모델에서 Residual Block을 입력된 개수만큼 짝지어서 하나의 layer로 구현할 예정이다.\n",
        "# channels : 각 layer를 빠져나올 때 출력되는 최종 channels\n",
        "\n",
        "resnet18_config = ResNetConfig(block=BasicBlock, n_blocks=[2,2,2,2], channels=[64,128,256,512])\n",
        "resnet34_config = ResNetConfig(block=BasicBlock, n_blocks=[3,4,6,3], channels=[64,128,256,512])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3fuythXNHymI"
      },
      "outputs": [],
      "source": [
        "# Resnet50, 101, 152 model에는 BottleNeck block을 사용한다\n",
        "# block : 모델에서 사용하는 블록의 종류 (BasicBlock이냐 BottleNeck Block이냐)\n",
        "# n_blocks : 모델에서 Residual Block을 입력된 개수만큼 짝지어서 하나의 layer로 구현할 예정이다.\n",
        "# channels : 각 layer를 빠져나올 때 출력되는 최종 channels\n",
        "\n",
        "resnet50_config = ResNetConfig(block=Bottleneck, n_blocks=[3,4,6,3], channels=[64,128,256,512])\n",
        "resnet101_config = ResNetConfig(block=Bottleneck, n_blocks=[3,4,23,3], channels=[64,128,256,512])\n",
        "resnet152_config = ResNetConfig(block=Bottleneck, n_blocks=[3,8,36,3], channels=[64,128,256,512])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "baDgosDk-IcM"
      },
      "outputs": [],
      "source": [
        "# 각 종류별 모델을 생성하는 함수를 제작한다\n",
        "output_dim = 10 # CIFAR10 Dataset의 클래스는 총 10개이다\n",
        "def ResNet18():\n",
        "    return ResNet(resnet18_config, output_dim)\n",
        "\n",
        "def ResNet34():\n",
        "    return ResNet(resnet34_config, output_dim)\n",
        "\n",
        "def ResNet50():\n",
        "    return ResNet(resnet50_config, output_dim)\n",
        "\n",
        "def ResNet101():\n",
        "    return ResNet(resnet101_config, output_dim)\n",
        "\n",
        "def ResNet152():\n",
        "    return ResNet(resnet152_config, output_dim)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pz_pu79Q5EsN"
      },
      "outputs": [],
      "source": [
        "# 모델 정의가 완료되었다\n",
        "# 학습 도중 Learning Rate를 지속적으로 조절하기 위한 Scheduler를 제작한다\n",
        "# 50 에폭 이상일 때는 Default learning rate의 10%\n",
        "# 100 에폭 이상일 때는 Dafault learning rate의 1%로 조절한다\n",
        "def lr_scheduler(optimizer, epoch):\n",
        "  lr = learning_rate\n",
        "  if epoch >= 50:\n",
        "    lr /= 10\n",
        "  if epoch >= 100:\n",
        "    lr /= 10\n",
        "  for param_group in optimizer.param_groups:\n",
        "    param_group['lr'] = lr\n",
        "\n",
        "# Dense Layer의 경우에는 가중치를 Xavier초기값으로 설정\n",
        "# 만약 전달된 객체가 nn.Linear를 상속한 계층이라면, 즉 Fully Connected Layer라면\n",
        "# 해당 모듈의 가중치를 xavier가중치로 초기화한다\n",
        "# 그리고 bias를 0.01로 초기화한다\n",
        "def init_weights(m):\n",
        "  if isinstance(m, nn.Linear):\n",
        "    torch.nn.init.xavier_uniform(m.weight)\n",
        "    m.bias.data.fill_(0.01)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rmg6SmJg8FR0",
        "outputId": "f52462f7-37f8-4dd9-d3d5-11cff582e375"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ]
        }
      ],
      "source": [
        "# Dataset을 Augmentation 및 Preprocess하기 위한 전처리기를 제작\n",
        "transform_train = transforms.Compose([\n",
        "  transforms.RandomCrop(32, padding=4), # 32 by 32사이즈로 Loading하고 각 border에 4의 패딩을 붙여 40 by 40 size를 나오게 한다\n",
        "  transforms.RandomHorizontalFlip(),    # 불러오는 데이터들을 무작위로 수평반전\n",
        "  transforms.ToTensor()                 # pixel값들을 정규화하고 Tensor자료형으로 변환시킨다\n",
        "])\n",
        "\n",
        "transform_test = transforms.Compose([\n",
        "  transforms.ToTensor()\n",
        "])\n",
        "\n",
        "# 모델의 Train 및 Test로 사용할 CIFAR10 Dataset를 불러온다\n",
        "train_dataset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform_train)\n",
        "test_dataset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform_test)\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=256, shuffle=True, num_workers=8)\n",
        "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=256, shuffle=False, num_workers=8)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4F8DMQWZ9mHN",
        "outputId": "66afe8d0-c327-4163-fa45-e45af88d232e"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:20: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n"
          ]
        }
      ],
      "source": [
        "# Device 및 model준비하기\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = ResNet50()\n",
        "# 모델의 Weight Initiallization에 앞서 정의한 함수를 이용한다\n",
        "model.apply(init_weights)\n",
        "model = model.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G4EYBHXK_UE9"
      },
      "outputs": [],
      "source": [
        "# Train을 위한 각종 변수 초기화하기\n",
        "learning_rate = 0.1\n",
        "num_epoch = 150\n",
        "model_name = \"model.pth\"\n",
        "\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "# 가중치를 0.0001이하로 떨어지지 않도록 다음과 같이 제한한다\n",
        "optimizer = optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9, weight_decay=0.0001)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "MThI6bBrAAys",
        "outputId": "21486db9-fb7f-4799-f19b-421fd83ea43d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "===== 1 epoch of 150 =====\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "====== 100 Step of 196 ======\n",
            "Train Acc : 0.37488397277227725\n",
            "Train Loss : 0.006810068618506193\n",
            "\n",
            "Valid Acc : 0.3728\n",
            "Valid Loss : 0.007474941201508045\n",
            "Model Saved!\n",
            "===== 2 epoch of 150 =====\n",
            "\n",
            "====== 100 Step of 196 ======\n",
            "Train Acc : 0.3954981435643564\n",
            "Train Loss : 0.006263743620365858\n",
            "\n",
            "Valid Acc : 0.426\n",
            "Valid Loss : 0.0067781987600028515\n",
            "Model Saved!\n",
            "===== 3 epoch of 150 =====\n",
            "\n",
            "====== 100 Step of 196 ======\n",
            "Train Acc : 0.4095374381188119\n",
            "Train Loss : 0.006532766856253147\n",
            "\n",
            "Valid Acc : 0.4519\n",
            "Valid Loss : 0.006374410819262266\n",
            "Model Saved!\n",
            "===== 4 epoch of 150 =====\n",
            "\n",
            "====== 100 Step of 196 ======\n",
            "Train Acc : 0.43019028465346537\n",
            "Train Loss : 0.006118895020335913\n",
            "\n",
            "Valid Acc : 0.4629\n",
            "Valid Loss : 0.006350953597575426\n",
            "Model Saved!\n",
            "===== 5 epoch of 150 =====\n",
            "\n",
            "====== 100 Step of 196 ======\n",
            "Train Acc : 0.43742264851485146\n",
            "Train Loss : 0.0055789886973798275\n",
            "\n",
            "Valid Acc : 0.4762\n",
            "Valid Loss : 0.006258875597268343\n",
            "Model Saved!\n",
            "===== 6 epoch of 150 =====\n",
            "\n",
            "====== 100 Step of 196 ======\n",
            "Train Acc : 0.44523514851485146\n",
            "Train Loss : 0.0058228811249136925\n",
            "\n",
            "Valid Acc : 0.4731\n",
            "Valid Loss : 0.006167639512568712\n",
            "===== 7 epoch of 150 =====\n",
            "\n",
            "====== 100 Step of 196 ======\n",
            "Train Acc : 0.4544399752475248\n",
            "Train Loss : 0.006074145436286926\n",
            "\n",
            "Valid Acc : 0.4863\n",
            "Valid Loss : 0.005736055783927441\n",
            "Model Saved!\n",
            "===== 8 epoch of 150 =====\n",
            "\n",
            "====== 100 Step of 196 ======\n",
            "Train Acc : 0.4694074876237624\n",
            "Train Loss : 0.005566837731748819\n",
            "\n",
            "Valid Acc : 0.4829\n",
            "Valid Loss : 0.005958526860922575\n",
            "===== 9 epoch of 150 =====\n",
            "\n",
            "====== 100 Step of 196 ======\n",
            "Train Acc : 0.47826423267326734\n",
            "Train Loss : 0.005799256730824709\n",
            "\n",
            "Valid Acc : 0.5131\n",
            "Valid Loss : 0.0053910305723547935\n",
            "Model Saved!\n",
            "===== 10 epoch of 150 =====\n",
            "\n",
            "====== 100 Step of 196 ======\n",
            "Train Acc : 0.4901763613861386\n",
            "Train Loss : 0.005163722671568394\n",
            "\n",
            "Valid Acc : 0.4983\n",
            "Valid Loss : 0.005528614856302738\n",
            "===== 11 epoch of 150 =====\n",
            "\n",
            "====== 100 Step of 196 ======\n",
            "Train Acc : 0.49261293316831684\n",
            "Train Loss : 0.0056199487298727036\n",
            "\n",
            "Valid Acc : 0.5173\n",
            "Valid Loss : 0.005377583205699921\n",
            "Model Saved!\n",
            "===== 12 epoch of 150 =====\n",
            "\n",
            "====== 100 Step of 196 ======\n",
            "Train Acc : 0.5033261138613861\n",
            "Train Loss : 0.005562235601246357\n",
            "\n",
            "Valid Acc : 0.493\n",
            "Valid Loss : 0.005902490578591824\n",
            "===== 13 epoch of 150 =====\n",
            "\n",
            "====== 100 Step of 196 ======\n",
            "Train Acc : 0.5120668316831684\n",
            "Train Loss : 0.00504293292760849\n",
            "\n",
            "Valid Acc : 0.5282\n",
            "Valid Loss : 0.005210580304265022\n",
            "Model Saved!\n",
            "===== 14 epoch of 150 =====\n",
            "\n",
            "====== 100 Step of 196 ======\n",
            "Train Acc : 0.5203821163366337\n",
            "Train Loss : 0.005002315156161785\n",
            "\n",
            "Valid Acc : 0.5359\n",
            "Valid Loss : 0.0051140873692929745\n",
            "Model Saved!\n",
            "===== 15 epoch of 150 =====\n",
            "\n",
            "====== 100 Step of 196 ======\n",
            "Train Acc : 0.5315980816831684\n",
            "Train Loss : 0.004914835095405579\n",
            "\n",
            "Valid Acc : 0.5455\n",
            "Valid Loss : 0.005030431784689426\n",
            "Model Saved!\n",
            "===== 16 epoch of 150 =====\n",
            "\n",
            "====== 100 Step of 196 ======\n",
            "Train Acc : 0.5397586633663366\n",
            "Train Loss : 0.00514636468142271\n",
            "\n",
            "Valid Acc : 0.5367\n",
            "Valid Loss : 0.005172364413738251\n",
            "===== 17 epoch of 150 =====\n",
            "\n",
            "====== 100 Step of 196 ======\n",
            "Train Acc : 0.5437035891089109\n",
            "Train Loss : 0.004350133705884218\n",
            "\n",
            "Valid Acc : 0.549\n",
            "Valid Loss : 0.00503355311229825\n",
            "Model Saved!\n",
            "===== 18 epoch of 150 =====\n",
            "\n",
            "====== 100 Step of 196 ======\n",
            "Train Acc : 0.5476098391089109\n",
            "Train Loss : 0.004563712514936924\n",
            "\n",
            "Valid Acc : 0.5494\n",
            "Valid Loss : 0.005039156414568424\n",
            "Model Saved!\n",
            "===== 19 epoch of 150 =====\n",
            "\n",
            "====== 100 Step of 196 ======\n",
            "Train Acc : 0.559637995049505\n",
            "Train Loss : 0.004926735535264015\n",
            "\n",
            "Valid Acc : 0.5536\n",
            "Valid Loss : 0.00506324740126729\n",
            "Model Saved!\n",
            "===== 20 epoch of 150 =====\n",
            "\n",
            "====== 100 Step of 196 ======\n",
            "Train Acc : 0.5629641089108911\n",
            "Train Loss : 0.004499567672610283\n",
            "\n",
            "Valid Acc : 0.571\n",
            "Valid Loss : 0.0049015530385077\n",
            "Model Saved!\n",
            "===== 21 epoch of 150 =====\n",
            "\n",
            "====== 100 Step of 196 ======\n",
            "Train Acc : 0.5781636757425742\n",
            "Train Loss : 0.004556752275675535\n",
            "\n",
            "Valid Acc : 0.5776\n",
            "Valid Loss : 0.004769311286509037\n",
            "Model Saved!\n",
            "===== 22 epoch of 150 =====\n",
            "\n",
            "====== 100 Step of 196 ======\n",
            "Train Acc : 0.5794012995049505\n",
            "Train Loss : 0.005158811341971159\n",
            "\n",
            "Valid Acc : 0.5611\n",
            "Valid Loss : 0.004975621588528156\n",
            "===== 23 epoch of 150 =====\n",
            "\n",
            "====== 100 Step of 196 ======\n",
            "Train Acc : 0.5890702351485149\n",
            "Train Loss : 0.004480216186493635\n",
            "\n",
            "Valid Acc : 0.5653\n",
            "Valid Loss : 0.004881993867456913\n",
            "===== 24 epoch of 150 =====\n",
            "\n",
            "====== 100 Step of 196 ======\n",
            "Train Acc : 0.5846225247524752\n",
            "Train Loss : 0.00464683398604393\n",
            "\n",
            "Valid Acc : 0.5886\n",
            "Valid Loss : 0.0047983210533857346\n",
            "Model Saved!\n",
            "===== 25 epoch of 150 =====\n",
            "\n",
            "====== 100 Step of 196 ======\n",
            "Train Acc : 0.6010210396039604\n",
            "Train Loss : 0.00421943049877882\n",
            "\n",
            "Valid Acc : 0.6013\n",
            "Valid Loss : 0.004655493889003992\n",
            "Model Saved!\n",
            "===== 26 epoch of 150 =====\n",
            "\n",
            "====== 100 Step of 196 ======\n",
            "Train Acc : 0.6021039603960396\n",
            "Train Loss : 0.004711061716079712\n",
            "\n",
            "Valid Acc : 0.6114\n",
            "Valid Loss : 0.004316077567636967\n",
            "Model Saved!\n",
            "===== 27 epoch of 150 =====\n",
            "\n",
            "====== 100 Step of 196 ======\n",
            "Train Acc : 0.6158725247524752\n",
            "Train Loss : 0.004058054182678461\n",
            "\n",
            "Valid Acc : 0.5922\n",
            "Valid Loss : 0.004747198428958654\n",
            "===== 28 epoch of 150 =====\n",
            "\n",
            "====== 100 Step of 196 ======\n",
            "Train Acc : 0.6197014232673267\n",
            "Train Loss : 0.004175884183496237\n",
            "\n",
            "Valid Acc : 0.6017\n",
            "Valid Loss : 0.004639826714992523\n",
            "===== 29 epoch of 150 =====\n",
            "\n",
            "====== 100 Step of 196 ======\n",
            "Train Acc : 0.6230275371287128\n",
            "Train Loss : 0.004034843295812607\n",
            "\n",
            "Valid Acc : 0.6165\n",
            "Valid Loss : 0.0043247165158391\n",
            "Model Saved!\n",
            "===== 30 epoch of 150 =====\n",
            "\n",
            "====== 100 Step of 196 ======\n",
            "Train Acc : 0.6388072400990099\n",
            "Train Loss : 0.004510076716542244\n",
            "\n",
            "Valid Acc : 0.6177\n",
            "Valid Loss : 0.0044267307966947556\n",
            "Model Saved!\n",
            "===== 31 epoch of 150 =====\n",
            "\n",
            "====== 100 Step of 196 ======\n",
            "Train Acc : 0.6398514851485149\n",
            "Train Loss : 0.0038204961456358433\n",
            "\n",
            "Valid Acc : 0.6311\n",
            "Valid Loss : 0.004160292912274599\n",
            "Model Saved!\n",
            "===== 32 epoch of 150 =====\n",
            "\n",
            "====== 100 Step of 196 ======\n",
            "Train Acc : 0.6477026608910891\n",
            "Train Loss : 0.004561608657240868\n",
            "\n",
            "Valid Acc : 0.5866\n",
            "Valid Loss : 0.00489360187202692\n",
            "===== 33 epoch of 150 =====\n",
            "\n",
            "====== 100 Step of 196 ======\n",
            "Train Acc : 0.6557472153465347\n",
            "Train Loss : 0.003849182976409793\n",
            "\n",
            "Valid Acc : 0.5675\n",
            "Valid Loss : 0.00499150063842535\n",
            "===== 34 epoch of 150 =====\n",
            "\n",
            "====== 100 Step of 196 ======\n",
            "Train Acc : 0.5749922648514851\n",
            "Train Loss : 0.0035715000703930855\n",
            "\n",
            "Valid Acc : 0.6107\n",
            "Valid Loss : 0.00443501491099596\n",
            "===== 35 epoch of 150 =====\n",
            "\n",
            "====== 100 Step of 196 ======\n",
            "Train Acc : 0.6486308787128713\n",
            "Train Loss : 0.003536753123626113\n",
            "\n",
            "Valid Acc : 0.6246\n",
            "Valid Loss : 0.004292399622499943\n",
            "===== 36 epoch of 150 =====\n",
            "\n",
            "====== 100 Step of 196 ======\n",
            "Train Acc : 0.6644879331683168\n",
            "Train Loss : 0.004062131512910128\n",
            "\n",
            "Valid Acc : 0.6622\n",
            "Valid Loss : 0.0038609481416642666\n",
            "Model Saved!\n",
            "===== 37 epoch of 150 =====\n",
            "\n",
            "====== 100 Step of 196 ======\n",
            "Train Acc : 0.6709854579207921\n",
            "Train Loss : 0.0036741788499057293\n",
            "\n",
            "Valid Acc : 0.6605\n",
            "Valid Loss : 0.0038977961521595716\n",
            "===== 38 epoch of 150 =====\n",
            "\n",
            "====== 100 Step of 196 ======\n",
            "Train Acc : 0.6751624381188119\n",
            "Train Loss : 0.003786108922213316\n",
            "\n",
            "Valid Acc : 0.1119\n",
            "Valid Loss : 1.591257095336914\n",
            "===== 39 epoch of 150 =====\n",
            "\n",
            "====== 100 Step of 196 ======\n",
            "Train Acc : 0.49168471534653463\n",
            "Train Loss : 0.00485695106908679\n",
            "\n",
            "Valid Acc : 0.5919\n",
            "Valid Loss : 0.004563579801470041\n",
            "===== 40 epoch of 150 =====\n",
            "\n",
            "====== 100 Step of 196 ======\n",
            "Train Acc : 0.6089882425742574\n",
            "Train Loss : 0.004163671750575304\n",
            "\n",
            "Valid Acc : 0.623\n",
            "Valid Loss : 0.004796881694346666\n",
            "===== 41 epoch of 150 =====\n",
            "\n",
            "====== 100 Step of 196 ======\n",
            "Train Acc : 0.6437964108910891\n",
            "Train Loss : 0.0032198461703956127\n",
            "\n",
            "Valid Acc : 0.6639\n",
            "Valid Loss : 0.003892435459420085\n",
            "Model Saved!\n",
            "===== 42 epoch of 150 =====\n",
            "\n",
            "====== 100 Step of 196 ======\n",
            "Train Acc : 0.6728418935643564\n",
            "Train Loss : 0.0034509492106735706\n",
            "\n",
            "Valid Acc : 0.6679\n",
            "Valid Loss : 0.0037688829470425844\n",
            "Model Saved!\n",
            "===== 43 epoch of 150 =====\n",
            "\n",
            "====== 100 Step of 196 ======\n",
            "Train Acc : 0.6770575495049505\n",
            "Train Loss : 0.00341482600197196\n",
            "\n",
            "Valid Acc : 0.6545\n",
            "Valid Loss : 0.004013403784483671\n",
            "===== 44 epoch of 150 =====\n",
            "\n",
            "====== 100 Step of 196 ======\n",
            "Train Acc : 0.6875773514851485\n",
            "Train Loss : 0.00361676886677742\n",
            "\n",
            "Valid Acc : 0.4785\n",
            "Valid Loss : 0.0072786808013916016\n",
            "===== 45 epoch of 150 =====\n",
            "\n",
            "====== 100 Step of 196 ======\n",
            "Train Acc : 0.6628248762376238\n",
            "Train Loss : 0.0031156372278928757\n",
            "\n",
            "Valid Acc : 0.6858\n",
            "Valid Loss : 0.003595591988414526\n",
            "Model Saved!\n",
            "===== 46 epoch of 150 =====\n",
            "\n",
            "====== 100 Step of 196 ======\n",
            "Train Acc : 0.7000309405940595\n",
            "Train Loss : 0.0032687890343368053\n",
            "\n",
            "Valid Acc : 0.6903\n",
            "Valid Loss : 0.003527561202645302\n",
            "Model Saved!\n",
            "===== 47 epoch of 150 =====\n",
            "\n",
            "====== 100 Step of 196 ======\n",
            "Train Acc : 0.710628094059406\n",
            "Train Loss : 0.003455153899267316\n",
            "\n",
            "Valid Acc : 0.708\n",
            "Valid Loss : 0.0033807449508458376\n",
            "Model Saved!\n",
            "===== 48 epoch of 150 =====\n",
            "\n",
            "====== 100 Step of 196 ======\n",
            "Train Acc : 0.7180925123762376\n",
            "Train Loss : 0.0031697647646069527\n",
            "\n",
            "Valid Acc : 0.7146\n",
            "Valid Loss : 0.003299783682450652\n",
            "Model Saved!\n",
            "===== 49 epoch of 150 =====\n",
            "\n",
            "====== 100 Step of 196 ======\n",
            "Train Acc : 0.7276454207920792\n",
            "Train Loss : 0.002646693727001548\n",
            "\n",
            "Valid Acc : 0.6972\n",
            "Valid Loss : 0.0034392683301120996\n",
            "===== 50 epoch of 150 =====\n",
            "\n",
            "====== 100 Step of 196 ======\n",
            "Train Acc : 0.7339495668316832\n",
            "Train Loss : 0.002799408044666052\n",
            "\n",
            "Valid Acc : 0.7256\n",
            "Valid Loss : 0.003259102813899517\n",
            "Model Saved!\n",
            "===== 51 epoch of 150 =====\n",
            "\n",
            "====== 100 Step of 196 ======\n",
            "Train Acc : 0.7592048267326733\n",
            "Train Loss : 0.002788012847304344\n",
            "\n",
            "Valid Acc : 0.7535\n",
            "Valid Loss : 0.002834025537595153\n",
            "Model Saved!\n",
            "===== 52 epoch of 150 =====\n",
            "\n",
            "====== 100 Step of 196 ======\n",
            "Train Acc : 0.7721225247524752\n",
            "Train Loss : 0.0028135154861956835\n",
            "\n",
            "Valid Acc : 0.7553\n",
            "Valid Loss : 0.002783295465633273\n",
            "Model Saved!\n",
            "===== 53 epoch of 150 =====\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-37-b67949678c0d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m     \u001b[0mpreds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1131\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-24-4190f35b8187>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     55\u001b[0m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmaxpool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer4\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1131\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    137\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    140\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1131\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-23-208e2e23e837>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0mi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbn1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1131\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/batchnorm.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    177\u001b[0m             \u001b[0mbn_training\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m             \u001b[0mexponential_average_factor\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 179\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    180\u001b[0m         )\n\u001b[1;32m    181\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mbatch_norm\u001b[0;34m(input, running_mean, running_var, weight, bias, training, momentum, eps)\u001b[0m\n\u001b[1;32m   2437\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2438\u001b[0m     return torch.batch_norm(\n\u001b[0;32m-> 2439\u001b[0;31m         \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrunning_mean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrunning_var\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmomentum\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackends\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcudnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menabled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2440\u001b[0m     )\n\u001b[1;32m   2441\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# Train 시작\n",
        "best_acc = 0\n",
        "\n",
        "for epoch in range(num_epoch):\n",
        "  print(\"=====\",\"{} epoch of {}\".format(epoch+1, num_epoch), \"=====\")\n",
        "  model.train()\n",
        "  lr_scheduler(optimizer, epoch)\n",
        "  train_loss = 0\n",
        "  valid_loss = 0\n",
        "  correct = 0\n",
        "  total_cnt = 0\n",
        "\n",
        "  for step, (x,y) in enumerate(train_loader):\n",
        "    x, y = x.to(device), y.to(device)\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    preds = model(x)\n",
        "    loss = loss_fn(preds, y)\n",
        "    loss.backward()\n",
        "\n",
        "    optimizer.step()\n",
        "    train_loss += loss.item()\n",
        "    predict = preds.max(1)[1]\n",
        "\n",
        "    total_cnt += y.size(0)\n",
        "    correct += predict.eq(y).sum().item()\n",
        "    \n",
        "    if step % 100 == 0 and step != 0:\n",
        "        print(f\"\\n====== { step } Step of { len(train_loader) } ======\")\n",
        "        print(f\"Train Acc : { correct / total_cnt }\")\n",
        "        print(f\"Train Loss : { loss.item() / y.size(0) }\")\n",
        "\n",
        "  correct = 0\n",
        "  total_cnt = 0\n",
        "  \n",
        "\n",
        "  # Test Data로 Epoch마다 성능평가하기\n",
        "  \n",
        "  with torch.no_grad():\n",
        "    model.eval()\n",
        "    for step, (x,y) in enumerate(test_loader):\n",
        "        # input and target\n",
        "        x, y = x.to(device), y.to(device)\n",
        "        total_cnt += y.size(0)\n",
        "        preds = model(x)\n",
        "        valid_loss += loss_fn(preds, y)\n",
        "        predict = preds.max(1)[1]\n",
        "        correct += predict.eq(y).sum().item()\n",
        "    valid_acc = correct / total_cnt\n",
        "    print(f\"\\nValid Acc : { valid_acc }\")    \n",
        "    print(f\"Valid Loss : { valid_loss / total_cnt }\")\n",
        "\n",
        "    if(valid_acc > best_acc):\n",
        "        best_acc = valid_acc\n",
        "        torch.save(model, model_name)\n",
        "        print(\"Model Saved!\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "ResNet (2015)",
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
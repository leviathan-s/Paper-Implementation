{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Seq2Seq (2014).ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# torchtext.legacy를 사용하기 위하여 아래 torchtext의 0.10.0 version을 설치한다\n",
        "!pip install torchtext==0.10.0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jzs-m6TykWOb",
        "outputId": "ff4e884c-c18c-4af5-9cf4-d75fb7cabb00"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting torchtext==0.10.0\n",
            "  Downloading torchtext-0.10.0-cp37-cp37m-manylinux1_x86_64.whl (7.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 7.6 MB 20.2 MB/s \n",
            "\u001b[?25hCollecting torch==1.9.0\n",
            "  Downloading torch-1.9.0-cp37-cp37m-manylinux1_x86_64.whl (831.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 831.4 MB 2.8 kB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torchtext==0.10.0) (1.21.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from torchtext==0.10.0) (4.64.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from torchtext==0.10.0) (2.23.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch==1.9.0->torchtext==0.10.0) (4.1.1)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.10.0) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.10.0) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.10.0) (2022.6.15)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.10.0) (2.10)\n",
            "Installing collected packages: torch, torchtext\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 1.12.1+cu113\n",
            "    Uninstalling torch-1.12.1+cu113:\n",
            "      Successfully uninstalled torch-1.12.1+cu113\n",
            "  Attempting uninstall: torchtext\n",
            "    Found existing installation: torchtext 0.13.1\n",
            "    Uninstalling torchtext-0.13.1:\n",
            "      Successfully uninstalled torchtext-0.13.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchvision 0.13.1+cu113 requires torch==1.12.1, but you have torch 1.9.0 which is incompatible.\n",
            "torchaudio 0.12.1+cu113 requires torch==1.12.1, but you have torch 1.9.0 which is incompatible.\u001b[0m\n",
            "Successfully installed torch-1.9.0 torchtext-0.10.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 필요한 Module을 import한다\n",
        "# 아래를 시작하기 위하여 kernel을 restart해야한다\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "from torchtext.legacy.datasets import Multi30k\n",
        "from torchtext.legacy.data import Field, BucketIterator\n",
        "import spacy\n",
        "import numpy as np\n",
        "\n",
        "import random\n",
        "import math\n",
        "import time"
      ],
      "metadata": {
        "id": "OvvP1gwOGdpU"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dataset은 PyTorch에서 제공하는 Multi30k dataset을 사용할 계획이다. "
      ],
      "metadata": {
        "id": "ihJH2zWHlOWm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 문장을 Tokenize하는 module을 설치한다\n",
        "!python -m spacy download en_core_web_sm\n",
        "!python -m spacy download de_core_news_sm"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VvqijDoglJvv",
        "outputId": "11ce545f-e560-439c-9ed0-8e3bc336ec92"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting en-core-web-sm==3.4.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.4.0/en_core_web_sm-3.4.0-py3-none-any.whl (12.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 12.8 MB 1.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: spacy<3.5.0,>=3.4.0 in /usr/local/lib/python3.7/dist-packages (from en-core-web-sm==3.4.0) (3.4.1)\n",
            "Requirement already satisfied: pathy>=0.3.5 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (0.6.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (21.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (2.4.4)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (1.21.6)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (1.0.3)\n",
            "Requirement already satisfied: thinc<8.2.0,>=8.1.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (8.1.0)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (1.0.8)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (57.4.0)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (2.0.8)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (3.3.0)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (2.23.0)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.10.0,>=1.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (1.9.2)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.9 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (3.0.10)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (0.10.1)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (3.0.7)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (4.64.0)\n",
            "Requirement already satisfied: typer<0.5.0,>=0.3.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (0.4.2)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (2.0.6)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (2.11.3)\n",
            "Requirement already satisfied: typing-extensions<4.2.0,>=3.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (4.1.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from catalogue<2.1.0,>=2.0.6->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (3.8.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (3.0.9)\n",
            "Requirement already satisfied: smart-open<6.0.0,>=5.2.1 in /usr/local/lib/python3.7/dist-packages (from pathy>=0.3.5->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (5.2.1)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (2022.6.15)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.7/dist-packages (from thinc<8.2.0,>=8.1.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (0.7.8)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.7/dist-packages (from typer<0.5.0,>=0.3.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (7.1.2)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (2.0.1)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting de-core-news-sm==3.4.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/de_core_news_sm-3.4.0/de_core_news_sm-3.4.0-py3-none-any.whl (14.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 14.6 MB 469 kB/s \n",
            "\u001b[?25hRequirement already satisfied: spacy<3.5.0,>=3.4.0 in /usr/local/lib/python3.7/dist-packages (from de-core-news-sm==3.4.0) (3.4.1)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->de-core-news-sm==3.4.0) (1.0.8)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->de-core-news-sm==3.4.0) (1.21.6)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->de-core-news-sm==3.4.0) (3.0.7)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->de-core-news-sm==3.4.0) (1.0.3)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.9 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->de-core-news-sm==3.4.0) (3.0.10)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->de-core-news-sm==3.4.0) (21.3)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->de-core-news-sm==3.4.0) (4.64.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->de-core-news-sm==3.4.0) (57.4.0)\n",
            "Requirement already satisfied: pathy>=0.3.5 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->de-core-news-sm==3.4.0) (0.6.2)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.10.0,>=1.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->de-core-news-sm==3.4.0) (1.9.2)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->de-core-news-sm==3.4.0) (2.23.0)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->de-core-news-sm==3.4.0) (2.4.4)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->de-core-news-sm==3.4.0) (2.0.6)\n",
            "Requirement already satisfied: thinc<8.2.0,>=8.1.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->de-core-news-sm==3.4.0) (8.1.0)\n",
            "Requirement already satisfied: typing-extensions<4.2.0,>=3.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->de-core-news-sm==3.4.0) (4.1.1)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->de-core-news-sm==3.4.0) (2.0.8)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->de-core-news-sm==3.4.0) (2.11.3)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->de-core-news-sm==3.4.0) (3.3.0)\n",
            "Requirement already satisfied: typer<0.5.0,>=0.3.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->de-core-news-sm==3.4.0) (0.4.2)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->de-core-news-sm==3.4.0) (0.10.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from catalogue<2.1.0,>=2.0.6->spacy<3.5.0,>=3.4.0->de-core-news-sm==3.4.0) (3.8.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->spacy<3.5.0,>=3.4.0->de-core-news-sm==3.4.0) (3.0.9)\n",
            "Requirement already satisfied: smart-open<6.0.0,>=5.2.1 in /usr/local/lib/python3.7/dist-packages (from pathy>=0.3.5->spacy<3.5.0,>=3.4.0->de-core-news-sm==3.4.0) (5.2.1)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->de-core-news-sm==3.4.0) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->de-core-news-sm==3.4.0) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->de-core-news-sm==3.4.0) (2022.6.15)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->de-core-news-sm==3.4.0) (1.24.3)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.7/dist-packages (from thinc<8.2.0,>=8.1.0->spacy<3.5.0,>=3.4.0->de-core-news-sm==3.4.0) (0.7.8)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.7/dist-packages (from typer<0.5.0,>=0.3.0->spacy<3.5.0,>=3.4.0->de-core-news-sm==3.4.0) (7.1.2)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->spacy<3.5.0,>=3.4.0->de-core-news-sm==3.4.0) (2.0.1)\n",
            "Installing collected packages: de-core-news-sm\n",
            "Successfully installed de-core-news-sm-3.4.0\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('de_core_news_sm')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenize Module import하기\n",
        "import de_core_news_sm\n",
        "import en_core_web_sm\n",
        "\n",
        "spacy_en = en_core_web_sm.load() # English Sentence Tokenize\n",
        "spacy_de = de_core_news_sm.load() # Destuch Sentence Tokenize\n",
        "\n",
        "# Tokenizer Function 생성하기\n",
        "# 독일어 문장을 입력받아 Tokenize한다\n",
        "# 논문에서는 입력문장을 Reverse order로 재 정렬하여 삽입하는데 이로 인해 text perplexity를 5.8에서 4.7로 감소시킨다\n",
        "def tokenize_de(text):\n",
        "  return [tok.text for tok in spacy_de.tokenizer(text)][::-1]\n",
        "\n",
        "# 영어 문장을 입력받아 Tokenize한다\n",
        "def tokenize_en(text):\n",
        "  return [tok.text for tok in spacy_en.tokenizer(text)]\n",
        "\n",
        "\n",
        "# Filed는 torchvision의 transforms.Compose와 같이\n",
        "# 데이터를 어떻게 처리하여 불러올 지 조절하는 메소드이다\n",
        "# 전처리 함수라고 볼 수 있음\n",
        "\n",
        "# source 데이터를 전처리할 때 사용하는 함수 정의\n",
        "SRC = Field(tokenize = tokenize_de,\n",
        "            init_token = \"<sos>\",\n",
        "            eos_token = \"<eos>\",\n",
        "            lower = True\n",
        "            )\n",
        "# target 데이터를 전처리할 때 사용하는 함수 정의\n",
        "TRG = Field(tokenize = tokenize_en,\n",
        "            init_token = \"<sos>\",\n",
        "            eos_token = \"<eos>\",\n",
        "            lower = True\n",
        "            )"
      ],
      "metadata": {
        "id": "HIbsC7GBlhUP"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset 불러오기\n",
        "# Multi 30K dataset을 이용하여 30000개의 영어, 독일어 문장을 다음과 같이 불러온다\n",
        "\n",
        "train_data, valid_data, test_data = Multi30k.splits(exts=('.de','.en'), fields=(SRC, TRG))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aJJDKAXWllw3",
        "outputId": "e7adc746-9760-4e84-863b-11de26e586c5"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "downloading training.tar.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "training.tar.gz: 100%|██████████| 1.21M/1.21M [00:03<00:00, 394kB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "downloading validation.tar.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "validation.tar.gz: 100%|██████████| 46.3k/46.3k [00:00<00:00, 116kB/s] \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "downloading mmt_task1_test2016.tar.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "mmt_task1_test2016.tar.gz: 100%|██████████| 66.2k/66.2k [00:00<00:00, 109kB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Number of training examples : {}\".format(len(train_data.examples)))\n",
        "print(\"Number of validation examples : {}\".format(len(valid_data.examples)))\n",
        "print(\"Number of test examples : {}\".format(len(test_data.examples)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i7O6raQUo2xg",
        "outputId": "3a9f0411-c732-4377-8c4a-bc995fa890cd"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of training examples : 29000\n",
            "Number of validation examples : 1014\n",
            "Number of test examples : 1000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# build Vocabulary Dictionary\n",
        "# 2회 이상 등장한 단어만 사용하여 단어사전을 만든다\n",
        "# 1번 이하 등장한 단어는 <unk>로 대체한다\n",
        "SRC.build_vocab(train_data, min_freq=2)\n",
        "TRG.build_vocab(train_data, min_freq=2)"
      ],
      "metadata": {
        "id": "b5OHR7zNpNLY"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Iterator를 생성한다\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "batch_size = 128\n",
        "\n",
        "# Bucket Iterator 모듈을 이용하면 다음과 같이 data를 batch개수만큼 불러오는 Dataloader와 비슷한\n",
        "# Iterator를 정의할 수 있다\n",
        "train_iterator, valid_iterator, test_iterator = BucketIterator.splits((train_data, valid_data, test_data), batch_size=batch_size, device=device)"
      ],
      "metadata": {
        "id": "C7Tuc5u1pqEn"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "hYre4Iz6FwAc"
      },
      "outputs": [],
      "source": [
        "# Encoder\n",
        "class Encoder(nn.Module):\n",
        "  def __init__(self, input_dim, emb_dim, hid_dim, n_layers, dropout):\n",
        "    super().__init__()\n",
        "\n",
        "    # input_dim : input data의 vocab_size = one-hot vector 크기\n",
        "    # embed_dim : 각각의 단어의 one-hot vector를 emb_dim길이의 dense_vector로 변환하는 것이다\n",
        "    # hid_dim : hidden_state의 차원 (=cell state의 차원)\n",
        "    # n_layers : LSTM의 Layer개수\n",
        "    # n_direction : 1 (단방향 LSTM 이므로)\n",
        "    self.hid_dim = hid_dim\n",
        "    self.n_layers = n_layers\n",
        "\n",
        "    # 단어의 input을 embbedding Vector로 변환\n",
        "    self.embedding = nn.Embedding(input_dim, emb_dim)\n",
        "\n",
        "    # Embedding Vector를 입력받아 hid_dim 크기의 hidden State와 Cell state출력\n",
        "    self.rnn = nn.LSTM(emb_dim, hid_dim, n_layers, dropout=dropout)\n",
        "\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "\n",
        "  def forward(self, src):\n",
        "    # source의 shape : (src_len, Batch_size)\n",
        "\n",
        "    # embedded된 단어 벡터\n",
        "    embedded = self.dropout(self.embedding(src))\n",
        "\n",
        "    outputs, (hidden, cell) = self.rnn(embedded)\n",
        "\n",
        "    # outputs : (src_len, batch_size, hid_dim * n_directions), encoder에서 출력되는 각 time_step마다의 hidden_state값들\n",
        "    # hidden : (n_layers * n_direction, batch_size, hid_dim),각 layer의 마지막 은닉 상태\n",
        "    # cell : (n_layers * n_direction, batch_size, hid_dim),각 layer의 마지막 셀 상태\n",
        "\n",
        "    return hidden, cell"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Decoder\n",
        "class Decoder(nn.Module):\n",
        "  def __init__(self, output_dim, emb_dim, hid_dim, n_layers, dropout):\n",
        "    super().__init__()\n",
        "\n",
        "    self.output_dim = output_dim # output vector의 Vocab_size\n",
        "    self.hid_dim = hid_dim\n",
        "    self.n_layers = n_layers\n",
        "\n",
        "    # 목적어의 단어를 입력받아 embbedding vector로 변환한다\n",
        "    self.embedding = nn.Embedding(output_dim, emb_dim)\n",
        "\n",
        "    self.rnn = nn.LSTM(emb_dim, hid_dim, n_layers, dropout=dropout)\n",
        "\n",
        "    self.fc_out = nn.Linear(hid_dim, output_dim)\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "  def forward(self, input, hidden, cell):\n",
        "    # input : (batch_size), Decoder의 첫 LSTM입력은 <SOS>이다 \n",
        "    # hidden : (n_layers * n_direction, batch_size, hid_dim), Encoder의 마지막 LSTM에서 나온 hidden state를 각 대응하는 layer에 입력 한다\n",
        "    # cell : (n_layers * n_direction, batch_size, hid_dim), Encoder의 마지막 LSTM에서 나온 cell state를 각 대응하는 layer에 입력 한다\n",
        "\n",
        "    input = input.unsqueeze(0) # (1, batch_size)만큼의 <SOS> 입력\n",
        "    \n",
        "    embedded = self.dropout(self.embedding(input)) # (1, batch_size, emb_dim)\n",
        "\n",
        "    output, (hidden, cell) = self.rnn(embedded, (hidden, cell))\n",
        "    # output: [seq len, batch_size, hid dim * n directions]\n",
        "    # hidden: [n layers * n directions, batch size, hid dim]\n",
        "    # cell: [n layers * n directions, batch size, hid dim]    \n",
        "\n",
        "    prediction = self.fc_out(output.squeeze(0)) # (batch_size, output_dim)\n",
        "\n",
        "    return prediction, hidden, cell"
      ],
      "metadata": {
        "id": "i5X6XuDJIz_V"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Teacher Forcing은 논문에서는 다루지 않지만, 기타 블로그의 구현 코드에서 다루고 있어 포함하였다. **이는 일정 확률로 Decoder의 t 시점 LSTM에 t-1시점의 예측단어를 입력하는 것이 아니라, target단어를 입력하는 것이다**.(원래 해당 자리에 와야 하는 단어).\n",
        "\n",
        " 이렇게 되면 학습 초기에 안정적이다. 왜냐하면, **학습 초기에는 틀린단어를 예측할 확률이 높은데**, 틀린 단어를 다음 시점에 입력하기 보다, 이를 교정하여 올바른 단어를 LSTM Cell에 입력해주는 것으로, 학습 초기 안정적인 학습을 기대할 수 있다"
      ],
      "metadata": {
        "id": "6cF2cHuTiPzm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# seq2seq\n",
        "class Seq2Seq(nn.Module):\n",
        "  def __init__(self, encoder, decoder, device):\n",
        "    super().__init__()\n",
        "\n",
        "    self.encoder = encoder\n",
        "    self.decoder = decoder\n",
        "    self.device = device\n",
        "\n",
        "    # encoder와 decoder의 hid_dim일 일치하지 않는 경우 Error를 발생시킨다\n",
        "    assert encoder.hid_dim == decoder.hid_dim, \\\n",
        "      \"Hidden dimensions of encoder decoder must be equal\"\n",
        "\n",
        "    # encoder와 decoder의 layer개수가 일치하지 않는 경우 Error를 발생\n",
        "    assert encoder.n_layers == decoder.n_layers, \\\n",
        "      \"Encoder and Decoder must have equal number of layers\" \n",
        "\n",
        "  def forward(self, src, trg, teacher_forcing_ratio = 0.5):\n",
        "    # src : (src_len, batch_size)\n",
        "    # trg : (trg_len, batch_size)\n",
        "\n",
        "    batch_size = trg.shape[1]\n",
        "    trg_len = trg.shape[0]\n",
        "    trg_vocab_size = self.decoder.output_dim # output vector의 Vocab_size   \n",
        "\n",
        "    # decoder의 output을 저장하기 위한 Tensor정의\n",
        "    outputs = torch.zeros(trg_len, batch_size, trg_vocab_size).to(self.device)\n",
        "\n",
        "    # initial hidden state\n",
        "    hidden, cell = self.encoder(src)\n",
        "\n",
        "    # 첫 번째 입력인 <sos> Token\n",
        "    input = trg[0,:]\n",
        "\n",
        "    # <eos>제외하고 trg_len-1만큼 반복한다\n",
        "    for t in range(1, trg_len):\n",
        "      output, hidden, cell = self.decoder(input, hidden, cell)\n",
        "\n",
        "      outputs[t] = output\n",
        "\n",
        "      # teacher forcing을 사용할지 말지 결정한다\n",
        "      teacher_force = random.random() < teacher_forcing_ratio\n",
        "\n",
        "      # 가장 높은 확률을 가진 값을 얻는다\n",
        "      top1 = output.argmax(1)\n",
        "      \n",
        "      # teacher forcing의 경우 다음 lstm에 target_token입력\n",
        "      input = trg[t] if teacher_force else top1\n",
        "\n",
        "    return outputs"
      ],
      "metadata": {
        "id": "Fw34-pW4NioV"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 하이퍼 파라미터 설정\n",
        "\n",
        "# english, destuch의 vocabulary개수\n",
        "input_dim = len(SRC.vocab)\n",
        "output_dim = len(TRG.vocab)\n",
        "\n",
        "# 단어의 Embedding Vector의 차원\n",
        "enc_emb_dim = 256\n",
        "dec_emb_dim = 256\n",
        "\n",
        "# Hidden_state 및 Cell State의 차원\n",
        "hid_dim = 512\n",
        "\n",
        "# layer개수\n",
        "n_layers = 2\n",
        "\n",
        "# Encoder / Decoder에 적용할 Dropout\n",
        "enc_dropout = 0.5\n",
        "dec_dropout = 0.5"
      ],
      "metadata": {
        "id": "gwA-CQz1qbmI"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# model을 생성한다\n",
        "enc = Encoder(input_dim, enc_emb_dim, hid_dim, n_layers, enc_dropout)\n",
        "dec = Decoder(output_dim, dec_emb_dim, hid_dim, n_layers, dec_dropout)\n",
        "\n",
        "model = Seq2Seq(enc, dec, device).to(device)"
      ],
      "metadata": {
        "id": "hWSRM2lRrC3j"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 가중치 초기화 함수\n",
        "def init_weights(model):\n",
        "  for name, param in model.named_parameters():\n",
        "    nn.init.uniform_(param.data, -0.08, 0.08) # -0.08에서 0.08사이의 uniform distribution\n",
        "\n",
        "model.apply(init_weights)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ATkTDiVWro_I",
        "outputId": "c64da0b0-7188-4e66-c1f2-41cb0d97d808"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Seq2Seq(\n",
              "  (encoder): Encoder(\n",
              "    (embedding): Embedding(7853, 256)\n",
              "    (rnn): LSTM(256, 512, num_layers=2, dropout=0.5)\n",
              "    (dropout): Dropout(p=0.5, inplace=False)\n",
              "  )\n",
              "  (decoder): Decoder(\n",
              "    (embedding): Embedding(5893, 256)\n",
              "    (rnn): LSTM(256, 512, num_layers=2, dropout=0.5)\n",
              "    (fc_out): Linear(in_features=512, out_features=5893, bias=True)\n",
              "    (dropout): Dropout(p=0.5, inplace=False)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 모델의 학습 가능한 파라미터의 수를 측정한다\n",
        "# model.parameters() 호출 결과, model의 각 layer의 learnable parameter Tensor가 반환되고\n",
        "# p.numel()은 해당 Tensor의 원소 개수를 count해준다\n",
        "\n",
        "def count_parameters(model):\n",
        "  return sum(p.numel() for p in model.paramters() if p.requires_grad)"
      ],
      "metadata": {
        "id": "2DQcb463sPLv"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# optimizer 설정\n",
        "optimizer = optim.Adam(model.parameters())\n",
        "\n",
        "# Loss function 정의하기\n",
        "# pad token : 모든 입력문장데이터 의 길이를 통일하기 위하여 \n",
        "# 짧은 문장의 앞 또는 뒤에 padding을 추가하여 동일한 길이로 만든다\n",
        "trg_pad_idx = TRG.vocab.stoi[TRG.pad_token] # pad_token의 index 반환\n",
        "criterion = nn.CrossEntropyLoss(ignore_index = trg_pad_idx) # pad에 해당하는 index는 무시"
      ],
      "metadata": {
        "id": "nXjyC8KQsbEZ"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 학습을 위한 함수를 정의하기\n",
        "# Epoch의 평균 손실함수값 반환\n",
        "def train(model, iterator, optimizer, criterion, clip):\n",
        "  model.train()\n",
        "  epoch_loss = 0\n",
        "\n",
        "  for i, batch in enumerate(iterator): # 1 epoch의 훈련\n",
        "    src = batch.src\n",
        "    trg = batch.trg\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    output = model(src, trg) # (trg_len, batch_size, output_dim)\n",
        "    output_dim = output.shape[-1]\n",
        "    output = output[1:].view(-1, output_dim) # loss계산을 위하여 1차원으로 변경\n",
        "\n",
        "    trg = trg[1:].view(-1) # loss 계산을 위해 1차원으로 변경\n",
        "\n",
        "    loss = criterion(output, trg)\n",
        "    loss.backward()\n",
        "\n",
        "    # 기울기 clipping 실시\n",
        "    torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
        "\n",
        "    optimizer.step()\n",
        "    epoch_loss += loss.item()\n",
        "\n",
        "  return epoch_loss / len(iterator)"
      ],
      "metadata": {
        "id": "u50ebhZEtd0p"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# evaluation 함수를 정의\n",
        "# 1 epoch을 평가하고 그 결과인 평균손실함수 반환\n",
        "def evaluate(model, iterator, criterion):\n",
        "  model.eval()\n",
        "  epoch_loss = 0\n",
        "\n",
        "  with torch.no_grad():\n",
        "    for i, batch in enumerate(iterator):\n",
        "      src = batch.src\n",
        "      trg = batch.trg\n",
        "\n",
        "      # output : (trg_len, batch_size, output_dim)\n",
        "      output = model(src, trg, 0) # Teacher forcing Off\n",
        "      output_dim = output.shape[-1]\n",
        "      \n",
        "      # (trg_len-1) * batch_size, output_dim\n",
        "      output = output[1:].view(-1, output_dim)\n",
        "\n",
        "      # (trg_len-1) * batch_size, output_dim\n",
        "      trg = trg[1:].view(-1)\n",
        "\n",
        "      loss = criterion(output, trg)\n",
        "      epoch_loss += loss.item()\n",
        "\n",
        "  return epoch_loss / len(iterator)\n"
      ],
      "metadata": {
        "id": "6wtAYuPmu_mg"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Time을 측정하기 위한 함수 정의\n",
        "def epoch_time(start_time, end_time):\n",
        "    elapsed_time = end_time - start_time\n",
        "    elapsed_mins = int(elapsed_time / 60)\n",
        "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
        "    return elapsed_mins, elapsed_secs"
      ],
      "metadata": {
        "id": "HbdAWvVKwCt5"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 학습 시작\n",
        "num_epochs = 10\n",
        "clip = 1\n",
        "\n",
        "best_valid_loss = float('inf')\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "   \n",
        "    start_time = time.time()\n",
        "    \n",
        "    train_loss = train(model, train_iterator, optimizer, criterion, clip)\n",
        "    valid_loss = evaluate(model, valid_iterator, criterion)\n",
        "    \n",
        "    end_time = time.time()\n",
        "    \n",
        "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
        "    \n",
        "    if valid_loss < best_valid_loss:\n",
        "        best_valid_loss = valid_loss\n",
        "        torch.save(model.state_dict(), 'tut1-model.pt')\n",
        "    \n",
        "    print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')\n",
        "    print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}')\n",
        "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. PPL: {math.exp(valid_loss):7.3f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "deI3oB6UwK3C",
        "outputId": "39cf1ae1-70c3-4f2f-a0a6-8804fbf3e1c7"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 01 | Time: 0m 37s\n",
            "\tTrain Loss: 5.066 | Train PPL: 158.524\n",
            "\t Val. Loss: 4.981 |  Val. PPL: 145.558\n",
            "Epoch: 02 | Time: 0m 36s\n",
            "\tTrain Loss: 4.491 | Train PPL:  89.202\n",
            "\t Val. Loss: 4.702 |  Val. PPL: 110.150\n",
            "Epoch: 03 | Time: 0m 37s\n",
            "\tTrain Loss: 4.171 | Train PPL:  64.763\n",
            "\t Val. Loss: 4.622 |  Val. PPL: 101.685\n",
            "Epoch: 04 | Time: 0m 37s\n",
            "\tTrain Loss: 3.982 | Train PPL:  53.635\n",
            "\t Val. Loss: 4.467 |  Val. PPL:  87.104\n",
            "Epoch: 05 | Time: 0m 37s\n",
            "\tTrain Loss: 3.814 | Train PPL:  45.316\n",
            "\t Val. Loss: 4.327 |  Val. PPL:  75.715\n",
            "Epoch: 06 | Time: 0m 37s\n",
            "\tTrain Loss: 3.651 | Train PPL:  38.503\n",
            "\t Val. Loss: 4.286 |  Val. PPL:  72.670\n",
            "Epoch: 07 | Time: 0m 37s\n",
            "\tTrain Loss: 3.525 | Train PPL:  33.948\n",
            "\t Val. Loss: 4.142 |  Val. PPL:  62.910\n",
            "Epoch: 08 | Time: 0m 37s\n",
            "\tTrain Loss: 3.365 | Train PPL:  28.942\n",
            "\t Val. Loss: 4.100 |  Val. PPL:  60.319\n",
            "Epoch: 09 | Time: 0m 36s\n",
            "\tTrain Loss: 3.266 | Train PPL:  26.211\n",
            "\t Val. Loss: 3.978 |  Val. PPL:  53.414\n",
            "Epoch: 10 | Time: 0m 37s\n",
            "\tTrain Loss: 3.147 | Train PPL:  23.270\n",
            "\t Val. Loss: 3.920 |  Val. PPL:  50.398\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "마지막으로 Test Data에 대한 성능 평가 시행"
      ],
      "metadata": {
        "id": "zuWguJJFzFeR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# best val loss일 때의 가중치를 불러온다\n",
        "model.load_state_dict(torch.load('tut1-model.pt'))\n",
        "\n",
        "# test loss를 측정한다\n",
        "test_loss = evaluate(model, test_iterator, criterion)\n",
        "\n",
        "print(f'| Test Loss: {test_loss:.3f} | Test PPL: {math.exp(test_loss):7.3f} |')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oT4sRDB6waud",
        "outputId": "b861d067-15f9-43b9-fefc-2ddeeb5a6073"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "| Test Loss: 3.913 | Test PPL:  50.025 |\n"
          ]
        }
      ]
    }
  ]
}
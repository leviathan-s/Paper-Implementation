{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "VGGNet",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tg90B0NAZlZc"
      },
      "outputs": [],
      "source": [
        "# VGGNet중 가장 간단한 모델인 VGG11을 구현할 예정이다.\n",
        "# 필요한 모듈 import\n",
        "\n",
        "import copy\n",
        "import numpy as np\n",
        "import time\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torch.utils.data as data\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.datasets as Datasets"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# VCG모델을 정의하는 클래스 제작\n",
        "# 입력의 feature에 따라서 VGG11, VGG13, VGG16, VGG19가 될 수 있다.\n",
        "\n",
        "class VGG(nn.Module):\n",
        "  def __init__(self, features, output_dim):\n",
        "    super(VGG, self).__init__()\n",
        "    self.features = features\n",
        "    self.avgpool = nn.AdaptiveAvgPool2d(7)\n",
        "    self.classifier = nn.Sequential(\n",
        "        nn.Linear(512*7*7, 4096),\n",
        "        nn.ReLU(inplace = True),\n",
        "        nn.Dropout(0.5),\n",
        "        nn.Linear(4096,4096),\n",
        "        nn.ReLU(inplace = True),\n",
        "        nn.Dropout(0.5),\n",
        "        nn.Linear(4096, output_dim)\n",
        "    )\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.features(x)\n",
        "    x = self.avgpool(x)\n",
        "    h = x.view(x.shape[0],-1)\n",
        "    x = self.classifier(h)\n",
        "    return x, h"
      ],
      "metadata": {
        "id": "I39XWVTIZ-pd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 각 VGG모델의 계층들을 정리한 것이다\n",
        "# M : Max Pooling을 의미한다\n",
        "# 숫자 : 입력 데이터를 해당 숫자의 채널값을 가진 feature map으로 변환해주는 Convolutional Layer를 의미한다\n",
        "\n",
        "vgg11_config = [64,'M',128,'M',256,256,'M',512,512,'M',512,512,'M']\n",
        "vgg13_config = [64,64,'M',128,128,'M',256,256,'M',512,512,'M',512,512,'M']\n",
        "vgg16_config = [64,64,'M',128,128,'M',256,256,256,'M',512,512,512,'M',512,512,512,'M']\n",
        "vgg19_config = [64,64,'M',128,128,'M',256,256,256,256,'M',512,512,512,512,'M',512,512,512,512,'M']"
      ],
      "metadata": {
        "id": "qStMhLZ3d19o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# VGG에서 convolution layer를 정의하기\n",
        "def get_vgg_layers(config, batch_norm):\n",
        "  layers = []\n",
        "  in_channels = 3\n",
        "\n",
        "  for c in config:\n",
        "    assert c == \"M\" or isinstance(c, int)\n",
        "    \n",
        "    # 만약 config에서 추출된 원소가 'M'이라면 shape를 반으로 감소시키는 최대풀링층을 추가한다 \n",
        "    if c == \"M\":\n",
        "      layers += [nn.MaxPool2d(kernel_size = 2)]\n",
        "    else:\n",
        "      conv2d = nn.Conv2d(in_channels, c, kernel_size=3, padding=1)\n",
        "\n",
        "      # 배치 정규화를 사용하도록 설정이 되어 있다면\n",
        "      # convolutional layer와 ReLU계층 사이에 Batch Normalization layer를 함께 추가시키도록 한다\n",
        "      if batch_norm:\n",
        "        layers += [conv2d, nn.BatchNorm2d(c), nn.ReLU(inplace=True)]\n",
        "      else:\n",
        "        layers += [conv2d, nn.ReLU(inplace=True)]\n",
        "      in_channels = c\n",
        "\n",
        "  return nn.Sequential(*layers)    "
      ],
      "metadata": {
        "id": "Ff5MDWrOgB0u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# VGG11 model의 앞부분을 생성한다.\n",
        "# 물론 해당 model은 VGG Network의 앞 부분인 convolution 계층에 해당한다\n",
        "# 후에 VGG클래스의 생성자에 이 모델을 삽입하여 완성된 VGG11모델을 반환할 것이다.\n",
        "vgg11_layers = get_vgg_layers(vgg11_config, batch_norm = True)"
      ],
      "metadata": {
        "id": "HIffGQPvisS1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 완성된 VGG11네트워크 생성하기\n",
        "OUTPUT_DIM = 2 # 개와 고양이 두 개의 클래스를 이용하므로\n",
        "model = VGG(vgg11_layers, OUTPUT_DIM)\n",
        "print(model)\n",
        "\n",
        "# 사전 훈련된 모델을 다음과 같이 생성해도 되지만, 직접 구현하기 위해 다음 방법을 사용하지는 않았다.\n",
        "# import torchvision.models as models\n",
        "# pretrained_model = models.vgg11_bn(pretrained = True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hQskNZfki-VH",
        "outputId": "ce9a1e9e-adc0-4d71-a7cc-895c6efb05ba"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VGG(\n",
            "  (features): Sequential(\n",
            "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (2): ReLU(inplace=True)\n",
            "    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (4): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (5): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (6): ReLU(inplace=True)\n",
            "    (7): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (8): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (9): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (10): ReLU(inplace=True)\n",
            "    (11): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (12): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (13): ReLU(inplace=True)\n",
            "    (14): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (15): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (16): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (17): ReLU(inplace=True)\n",
            "    (18): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (19): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (20): ReLU(inplace=True)\n",
            "    (21): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (22): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (23): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (24): ReLU(inplace=True)\n",
            "    (25): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (26): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (27): ReLU(inplace=True)\n",
            "    (28): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  )\n",
            "  (avgpool): AdaptiveAvgPool2d(output_size=7)\n",
            "  (classifier): Sequential(\n",
            "    (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
            "    (1): ReLU(inplace=True)\n",
            "    (2): Dropout(p=0.5, inplace=False)\n",
            "    (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
            "    (4): ReLU(inplace=True)\n",
            "    (5): Dropout(p=0.5, inplace=False)\n",
            "    (6): Linear(in_features=4096, out_features=2, bias=True)\n",
            "  )\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 모델을 완결하였으니 본격적으로 이미지에 대한 전처리를 위한 코드를 작성하자\n",
        "train_transforms = transforms.Compose([\n",
        "                  transforms.Resize((256,256)),\n",
        "                  transforms.RandomRotation(5), # 주어진 이미지 데이터를 5 degree 이하로 회전시키는 data augmentation\n",
        "                  transforms.RandomHorizontalFlip(0.5),\n",
        "                  transforms.ToTensor(),\n",
        "                  transforms.Normalize(mean=[0.485,0.456,0.406],std=[0.229,0.224,0.225])\n",
        "])\n",
        "\n",
        "test_transforms = transforms.Compose([\n",
        "                  transforms.Resize((256,256)),\n",
        "                  transforms.ToTensor(),\n",
        "                  transforms.Normalize(mean=[0.485,0.456,0.406],std=[0.229,0.224,0.225])\n",
        "])"
      ],
      "metadata": {
        "id": "oFZT9zHSj3Qt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 데이터셋을 정리하기\n",
        "# train 디렉토리와 test 디렉토리가 존재한다\n",
        "# train, test 디렉토리 각각에 Cat, Dog 디렉토리가 있다.\n",
        "\n",
        "train_path = \"train\"\n",
        "test_path = \"test\"\n",
        "\n",
        "train_dataset = torchvision.datasets.ImageFolder(train_path, transform = train_transforms)\n",
        "test_dataset = torchvision.datasets.ImageFolder(test_path, transform = test_transforms)\n",
        "\n",
        "print(len(train_dataset))"
      ],
      "metadata": {
        "id": "80U3v4YqljSv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 훈련데이터와 검증데이터를 9대 1의 비율로 분할한다\n",
        "# 그리고 검증데이터를 미리 test_transforms로 전처리 한다.\n",
        "\n",
        "VALID_RATIO = 0.9\n",
        "n_train_examples = int(len(train_dataset) * VALID_RATIO)\n",
        "n_valid_examples = len(train_dataset) - n_train_examples\n",
        "\n",
        "train_data, valid_data = data.random_split(train_dataset, [n_train_examples, n_valid_examples])\n",
        "\n",
        "valid_data = copy.deepcopy(valid_data)\n",
        "valid_data.dataset.transform = test_transforms"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MvRntN7Gv0Td",
        "outputId": "cccbad7b-05c8-4764-b25f-9705c35d8c9d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['.config', 'test', '.ipynb_checkpoints', 'train', 'sample_data']"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 데이터로더를 이용하여 데이터셋의 데이터를 메모리로 가져온다\n",
        "BATCH_SIZE = 128\n",
        "\n",
        "train_iterator = data.DataLoader(train_data, shuffle = True, batch_size = BATCH_SIZE)\n",
        "valid_iterator = data.DataLoader(valid_data, shuffle = True, batch_size = BATCH_SIZE)\n",
        "test_iterator = data.DataLoader(test_dataset, shuffle = True, batch_size = BATCH_SIZE)"
      ],
      "metadata": {
        "id": "e4aolwZsw6JW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 모델학습에서 사용할 optimizer와 손실함수를 정의하기\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "optimizer = optim.Adam(model.parameters(), lr = 1e-7)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "model = model.to(device)\n",
        "criterion = criterion.to(device)"
      ],
      "metadata": {
        "id": "QjvuOug7xuYm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 모델의 정확도를 측정하기 위한 함수를 정의하기\n",
        "def calculate_accuracy(y_pred, y):\n",
        "  top_pred = y_pred.argmax(1, keepdim=True)\n",
        "  correct = top_pred.eq(y.view_as(top_pred)).sum() # 총 정답 개수\n",
        "  acc = correct.float() / y.shape[0] # 총 정답 개수를 배치 개수만큼 나누어준다\n",
        "  return acc"
      ],
      "metadata": {
        "id": "Jhzb7XjXyFwt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 모델 학습을 위한 함수를 정의하기\n",
        "# 1 epoch를 훈련하는 함수이다\n",
        "def train(model, iterator, optimizer, criterion, device):\n",
        "  \n",
        "  epoch_loss = 0\n",
        "  epoch_acc = 0\n",
        "  \n",
        "  model.train()\n",
        "  # train_dataloader로부터 한번에 배치 개수만큼 데이터를 불러오는 루틴이다. \n",
        "  for (x, y) in iterator:\n",
        "    x, y = x.to(device), y.to(device)\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    y_pred, _ = model(x) # 아까 모델에서 2개의 값을 반환한다고 정의하였었다. 각 클래스별 확률값은 첫 번째 반환값으로 설정하였다.\n",
        "    loss = criterion(y_pred, y)\n",
        "    acc = calculate_accuracy(y_pred, y)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    # 평균 정확도와 평균 손실함수값을 축적해나간다\n",
        "    epoch_loss += loss.item()\n",
        "    epoch_acc += acc.item()\n",
        "\n",
        "  return epoch_loss / len(iterator), epoch_acc / len(iterator)"
      ],
      "metadata": {
        "id": "kiDQlIZ6y-st"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 모델의 성능을 측정하는 함수\n",
        "def evaluate(model, iterator, criterion, device):\n",
        "  epoch_loss = 0\n",
        "  epoch_acc = 0\n",
        "\n",
        "  model.eval()\n",
        "  with torch.no_grad():\n",
        "    for (x, y) in iterator:\n",
        "      x, y = x.to(device), y.to(device)\n",
        "      y_pred, _ = model(x)\n",
        "      loss = criterion(y_pred, y)\n",
        "      acc = calculate_accuracy(y_pred, y)\n",
        "      epoch_loss += loss.item()\n",
        "      epoch_acc += acc.item()\n",
        "\n",
        "  return epoch_loss / len(iterator), epoch_acc / len(iterator)"
      ],
      "metadata": {
        "id": "VnEv8RnE0Zje"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 모델의 학습시간을 측정하기 위한 함수\n",
        "def epoch_time(start_time, end_time):\n",
        "  elapsed_time = end_time - start_time\n",
        "  elapsed_mins = int(elapsed_time / 60)\n",
        "  elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
        "  return elapsed_mins, elapsed_secs"
      ],
      "metadata": {
        "id": "LjVilhnI1KUK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 모델을 학습\n",
        "EPOCHS = 5\n",
        "best_valid_loss = float('inf')\n",
        "for epoch in range(EPOCHS):\n",
        "  start_time = time.monotonic() # 현재 OS에서의 시간값 반환\n",
        "  train_loss, train_acc = train(model, train_iterator, optimizer, criterion, device)\n",
        "  valid_loss, valid_acc = evaluate(model, valid_iterator, criterion, device)\n",
        "\n",
        "  if valid_loss < best_valid_loss:\n",
        "    best_valid_loss = valid_loss\n",
        "    torch.save(model.state_dict(),\"./best_VGG_model.pt\")\n",
        "\n",
        "  end_time = time.monotonic()\n",
        "  epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
        "\n",
        "  print(f\"EPOCH : {epoch+1} | Epoch Time : {epoch_mins}m {epoch_secs}s\")\n",
        "  print(f\"Train Loss : {train_loss} | Train Acc : {train_acc}\")\n",
        "  print(f\"Valid Loss : {valid_loss} | Valid Acc : {valid_acc}\")\n"
      ],
      "metadata": {
        "id": "ZWVzBrfI1rA_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 앞서 저장했던 손실함수값이 가장 낮았던 모델을 불러와서 Test dataset으로 성능을 측정해보자\n",
        "model.load_state_dict(\"./best_VGG_model.pt\")\n",
        "test_loss, test_acc = evaluate(model, test_iterator, criterion, device)\n",
        "print(f\"Test Loss : {test_loss} | Test Acc : {test_acc}\")\n",
        "\n",
        "\n",
        "# Test데이터셋을 이용한 모델 성능 측정 결과\n",
        "# Test loss는 0.684\n",
        "# Test Acc는 0.5로 결과가 좋지 않다.\n",
        "# 수많은 가중치가 있는데 반해 데이터가 너무 적기 때문이다.\n",
        "# 데이터셋을 충분히 늘려야 성능이 좋아질것이다."
      ],
      "metadata": {
        "id": "6e6aZjv12tc4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 테스트 데이터셋을 이용하여 모델의 예측 결과를 알아보기 위한 함수를 정의한다\n",
        "# test_dataset의 모든 데이터에 대해 모델이 예측한 이미지 원본, 정답 label값, 확률값을 반환한다.\n",
        "def get_predictions(model, iterator):\n",
        "  model.eval()\n",
        "  images = []\n",
        "  labels = []\n",
        "  probs = []\n",
        "  with torch.no_grad():\n",
        "    for (x, y) in iterator:\n",
        "      x, y = x.to(device), y.to(device)\n",
        "      y_pred, _ = model(x)\n",
        "      y_prob = F.softmax(y_pred, dim=1)\n",
        "      top_pred = y_prob.argmax(1, keepdim = True)\n",
        "\n",
        "      # image, label, prob모두 matplotlib에서 사용될 것이기 때문에 cpu로 옮겨놓는다\n",
        "      images.append(x.cpu())\n",
        "      labels.append(y.cpu()) \n",
        "      probs.append(y_prob.cpu())\n",
        "\n",
        "\n",
        "  images = torch.cat(images, dim=0)\n",
        "  labels = torch.cat(labels, dim=0)\n",
        "  probs = torch.cat(probs, dim=0)\n",
        "\n",
        "  return images, labels, probs"
      ],
      "metadata": {
        "id": "qG3Je98w5-Hg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "images, labels, probs = get_predictions(model, test_iterator)\n",
        "# 차례대로 이미지 원본, 해당 이미지의 정답 레이블, 모델이 예측한 확률값이다.\n",
        "pred_labels = torch.argmax(probs, 1)\n",
        "corrects = torch.eq(labels, pred_labels)\n",
        "correct_examples = []\n",
        "\n",
        "# 모델이 정답을 맞춘 이미지를 저장한다\n",
        "for image, label, prob, correct in zip(images, labels, probs, corrects):\n",
        "  if correct:\n",
        "    correct_examples.append((image, label, prob))\n",
        "\n",
        "correct_example.sort(reverse=True, key=lambda x:torch.max(x[2], dim=0).values)"
      ],
      "metadata": {
        "id": "FkyaJZC87BWw"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
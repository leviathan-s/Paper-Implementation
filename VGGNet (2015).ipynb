{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"VGGNet","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyMonGdVaihXU4u9/UG1ar+U"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":28,"metadata":{"id":"tg90B0NAZlZc","executionInfo":{"status":"ok","timestamp":1658073054577,"user_tz":-540,"elapsed":274,"user":{"displayName":"Drive Google","userId":"18351879974904137523"}}},"outputs":[],"source":["# VGGNet중 가장 간단한 모델인 VGG11을 구현할 예정이다.\n","# 필요한 모듈 import\n","\n","import copy\n","import numpy as np\n","import time\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","import torch.utils.data as data\n","import torchvision\n","import torchvision.transforms as transforms\n","import torchvision.datasets as Datasets"]},{"cell_type":"code","source":["# VCG모델을 정의하는 클래스 제작\n","# 입력의 feature에 따라서 VGG11, VGG13, VGG16, VGG19가 될 수 있다.\n","\n","class VGG(nn.Module):\n","  def __init__(self, features, output_dim):\n","    super(VGG, self).__init__()\n","    self.features = features\n","    self.avgpool = nn.AdaptiveAvgPool2d(7)\n","    self.classifier = nn.Sequential(\n","        nn.Linear(512*7*7, 4096),\n","        nn.ReLU(inplace = True),\n","        nn.Dropout(0.5),\n","        nn.Linear(4096,4096),\n","        nn.ReLU(inplace = True),\n","        nn.Dropout(0.5),\n","        nn.Linear(4096, output_dim)\n","    )\n","\n","  def forward(self, x):\n","    x = self.features(x)\n","    x = self.avgpool(x)\n","    h = x.view(x.shape[0],-1)\n","    x = self.classifier(h)\n","    return x, h"],"metadata":{"id":"I39XWVTIZ-pd","executionInfo":{"status":"ok","timestamp":1658068034252,"user_tz":-540,"elapsed":12,"user":{"displayName":"Drive Google","userId":"18351879974904137523"}}},"execution_count":9,"outputs":[]},{"cell_type":"code","source":["# 각 VGG모델의 계층들을 정리한 것이다\n","# M : Max Pooling을 의미한다\n","# 숫자 : 입력 데이터를 해당 숫자의 채널값을 가진 feature map으로 변환해주는 Convolutional Layer를 의미한다\n","\n","vgg11_config = [64,'M',128,'M',256,256,'M',512,512,'M',512,512,'M']\n","vgg13_config = [64,64,'M',128,128,'M',256,256,'M',512,512,'M',512,512,'M']\n","vgg16_config = [64,64,'M',128,128,'M',256,256,256,'M',512,512,512,'M',512,512,512,'M']\n","vgg19_config = [64,64,'M',128,128,'M',256,256,256,256,'M',512,512,512,512,'M',512,512,512,512,'M']"],"metadata":{"id":"qStMhLZ3d19o","executionInfo":{"status":"ok","timestamp":1658068034253,"user_tz":-540,"elapsed":12,"user":{"displayName":"Drive Google","userId":"18351879974904137523"}}},"execution_count":10,"outputs":[]},{"cell_type":"code","source":["# VGG에서 convolution layer를 정의하기\n","def get_vgg_layers(config, batch_norm):\n","  layers = []\n","  in_channels = 3\n","\n","  for c in config:\n","    assert c == \"M\" or isinstance(c, int)\n","    \n","    # 만약 config에서 추출된 원소가 'M'이라면 shape를 반으로 감소시키는 최대풀링층을 추가한다 \n","    if c == \"M\":\n","      layers += [nn.MaxPool2d(kernel_size = 2)]\n","    else:\n","      conv2d = nn.Conv2d(in_channels, c, kernel_size=3, padding=1)\n","\n","      # 배치 정규화를 사용하도록 설정이 되어 있다면\n","      # convolutional layer와 ReLU계층 사이에 Batch Normalization layer를 함께 추가시키도록 한다\n","      if batch_norm:\n","        layers += [conv2d, nn.BatchNorm2d(c), nn.ReLU(inplace=True)]\n","      else:\n","        layers += [conv2d, nn.ReLU(inplace=True)]\n","      in_channels = c\n","\n","  return nn.Sequential(*layers)    "],"metadata":{"id":"Ff5MDWrOgB0u","executionInfo":{"status":"ok","timestamp":1658068034254,"user_tz":-540,"elapsed":11,"user":{"displayName":"Drive Google","userId":"18351879974904137523"}}},"execution_count":11,"outputs":[]},{"cell_type":"code","source":["# VGG11 model의 앞부분을 생성한다.\n","# 물론 해당 model은 VGG Network의 앞 부분인 convolution 계층에 해당한다\n","# 후에 VGG클래스의 생성자에 이 모델을 삽입하여 완성된 VGG11모델을 반환할 것이다.\n","vgg11_layers = get_vgg_layers(vgg11_config, batch_norm = True)"],"metadata":{"id":"HIffGQPvisS1","executionInfo":{"status":"ok","timestamp":1658068034561,"user_tz":-540,"elapsed":317,"user":{"displayName":"Drive Google","userId":"18351879974904137523"}}},"execution_count":12,"outputs":[]},{"cell_type":"code","source":["# 완성된 VGG11네트워크 생성하기\n","OUTPUT_DIM = 2 # 개와 고양이 두 개의 클래스를 이용하므로\n","model = VGG(vgg11_layers, OUTPUT_DIM)\n","print(model)\n","\n","# 사전 훈련된 모델을 다음과 같이 생성해도 되지만, 직접 구현하기 위해 다음 방법을 사용하지는 않았다.\n","# import torchvision.models as models\n","# pretrained_model = models.vgg11_bn(pretrained = True)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hQskNZfki-VH","executionInfo":{"status":"ok","timestamp":1658068035802,"user_tz":-540,"elapsed":1249,"user":{"displayName":"Drive Google","userId":"18351879974904137523"}},"outputId":"ce9a1e9e-adc0-4d71-a7cc-895c6efb05ba"},"execution_count":13,"outputs":[{"output_type":"stream","name":"stdout","text":["VGG(\n","  (features): Sequential(\n","    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (2): ReLU(inplace=True)\n","    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n","    (4): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (5): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (6): ReLU(inplace=True)\n","    (7): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n","    (8): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (9): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (10): ReLU(inplace=True)\n","    (11): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (12): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (13): ReLU(inplace=True)\n","    (14): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n","    (15): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (16): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (17): ReLU(inplace=True)\n","    (18): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (19): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (20): ReLU(inplace=True)\n","    (21): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n","    (22): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (23): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (24): ReLU(inplace=True)\n","    (25): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (26): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (27): ReLU(inplace=True)\n","    (28): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n","  )\n","  (avgpool): AdaptiveAvgPool2d(output_size=7)\n","  (classifier): Sequential(\n","    (0): Linear(in_features=25088, out_features=4096, bias=True)\n","    (1): ReLU(inplace=True)\n","    (2): Dropout(p=0.5, inplace=False)\n","    (3): Linear(in_features=4096, out_features=4096, bias=True)\n","    (4): ReLU(inplace=True)\n","    (5): Dropout(p=0.5, inplace=False)\n","    (6): Linear(in_features=4096, out_features=2, bias=True)\n","  )\n",")\n"]}]},{"cell_type":"code","source":["# 모델을 완결하였으니 본격적으로 이미지에 대한 전처리를 위한 코드를 작성하자\n","train_transforms = transforms.Compose([\n","                  transforms.Resize((256,256)),\n","                  transforms.RandomRotation(5), # 주어진 이미지 데이터를 5 degree 이하로 회전시키는 data augmentation\n","                  transforms.RandomHorizontalFlip(0.5),\n","                  transforms.ToTensor(),\n","                  transforms.Normalize(mean=[0.485,0.456,0.406],std=[0.229,0.224,0.225])\n","])\n","\n","test_transforms = transforms.Compose([\n","                  transforms.Resize((256,256)),\n","                  transforms.ToTensor(),\n","                  transforms.Normalize(mean=[0.485,0.456,0.406],std=[0.229,0.224,0.225])\n","])"],"metadata":{"id":"oFZT9zHSj3Qt","executionInfo":{"status":"ok","timestamp":1658068544497,"user_tz":-540,"elapsed":287,"user":{"displayName":"Drive Google","userId":"18351879974904137523"}}},"execution_count":14,"outputs":[]},{"cell_type":"code","source":["# 데이터셋을 정리하기\n","# train 디렉토리와 test 디렉토리가 존재한다\n","# train, test 디렉토리 각각에 Cat, Dog 디렉토리가 있다.\n","\n","train_path = \"train\"\n","test_path = \"test\"\n","\n","train_dataset = torchvision.datasets.ImageFolder(train_path, transform = train_transforms)\n","test_dataset = torchvision.datasets.ImageFolder(test_path, transform = test_transforms)\n","\n","print(len(train_dataset))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":990},"id":"80U3v4YqljSv","executionInfo":{"status":"error","timestamp":1658071336896,"user_tz":-540,"elapsed":273,"user":{"displayName":"Drive Google","userId":"18351879974904137523"}},"outputId":"0fe38aa4-a78f-4fef-c5c4-6397e9addd3b"},"execution_count":20,"outputs":[{"output_type":"error","ename":"FileNotFoundError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-20-1b72756b65be>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mtest_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"test\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mtrain_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorchvision\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatasets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mImageFolder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransform\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_transforms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mtest_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorchvision\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatasets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mImageFolder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransform\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_transforms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, root, transform, target_transform, loader, is_valid_file)\u001b[0m\n\u001b[1;32m    314\u001b[0m             \u001b[0mtransform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    315\u001b[0m             \u001b[0mtarget_transform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtarget_transform\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 316\u001b[0;31m             \u001b[0mis_valid_file\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mis_valid_file\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    317\u001b[0m         )\n\u001b[1;32m    318\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimgs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, root, loader, extensions, transform, target_transform, is_valid_file)\u001b[0m\n\u001b[1;32m    144\u001b[0m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_transform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtarget_transform\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m         \u001b[0mclasses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_to_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind_classes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 146\u001b[0;31m         \u001b[0msamples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_to_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mextensions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_valid_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    147\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloader\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36mmake_dataset\u001b[0;34m(directory, class_to_idx, extensions, is_valid_file)\u001b[0m\n\u001b[1;32m    188\u001b[0m             \u001b[0;31m# is potentially overridden and thus could have a different logic.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"The class_to_idx parameter cannot be None.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 190\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mmake_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirectory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_to_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mextensions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mextensions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_valid_file\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mis_valid_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    191\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfind_classes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdirectory\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTuple\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36mmake_dataset\u001b[0;34m(directory, class_to_idx, extensions, is_valid_file)\u001b[0m\n\u001b[1;32m    101\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mextensions\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m             \u001b[0mmsg\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34mf\"Supported extensions are: {extensions if isinstance(extensions, str) else ', '.join(extensions)}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mFileNotFoundError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0minstances\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mFileNotFoundError\u001b[0m: Found no valid file for the classes .ipynb_checkpoints. Supported extensions are: .jpg, .jpeg, .png, .ppm, .bmp, .pgm, .tif, .tiff, .webp"]}]},{"cell_type":"code","source":["# 훈련데이터와 검증데이터를 9대 1의 비율로 분할한다\n","# 그리고 검증데이터를 미리 test_transforms로 전처리 한다.\n","\n","VALID_RATIO = 0.9\n","n_train_examples = int(len(train_dataset) * VALID_RATIO)\n","n_valid_examples = len(train_dataset) - n_train_examples\n","\n","train_data, valid_data = data.random_split(train_dataset, [n_train_examples, n_valid_examples])\n","\n","valid_data = copy.deepcopy(valid_data)\n","valid_data.dataset.transform = test_transforms"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"MvRntN7Gv0Td","executionInfo":{"status":"ok","timestamp":1658071251098,"user_tz":-540,"elapsed":269,"user":{"displayName":"Drive Google","userId":"18351879974904137523"}},"outputId":"cccbad7b-05c8-4764-b25f-9705c35d8c9d"},"execution_count":19,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['.config', 'test', '.ipynb_checkpoints', 'train', 'sample_data']"]},"metadata":{},"execution_count":19}]},{"cell_type":"code","source":["# 데이터로더를 이용하여 데이터셋의 데이터를 메모리로 가져온다\n","BATCH_SIZE = 128\n","\n","train_iterator = data.DataLoader(train_data, shuffle = True, batch_size = BATCH_SIZE)\n","valid_iterator = data.DataLoader(valid_data, shuffle = True, batch_size = BATCH_SIZE)\n","test_iterator = data.DataLoader(test_dataset, shuffle = True, batch_size = BATCH_SIZE)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":303},"id":"e4aolwZsw6JW","executionInfo":{"status":"error","timestamp":1658071734352,"user_tz":-540,"elapsed":328,"user":{"displayName":"Drive Google","userId":"18351879974904137523"}},"outputId":"264ea641-bf62-4d38-e0ca-d1b2285fff04"},"execution_count":21,"outputs":[{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-21-419e15dee1f5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mBATCH_SIZE\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m128\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mtrain_iterator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mvalid_iterator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalid_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mtest_iterator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'train_data' is not defined"]}]},{"cell_type":"code","source":["# 모델학습에서 사용할 optimizer와 손실함수를 정의하기\n","device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","\n","optimizer = optim.Adam(model.parameters(), lr = 1e-7)\n","criterion = nn.CrossEntropyLoss()\n","\n","model = model.to(device)\n","criterion = criterion.to(device)"],"metadata":{"id":"QjvuOug7xuYm"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 모델의 정확도를 측정하기 위한 함수를 정의하기\n","def calculate_accuracy(y_pred, y):\n","  top_pred = y_pred.argmax(1, keepdim=True)\n","  correct = top_pred.eq(y.view_as(top_pred)).sum() # 총 정답 개수\n","  acc = correct.float() / y.shape[0] # 총 정답 개수를 배치 개수만큼 나누어준다\n","  return acc"],"metadata":{"id":"Jhzb7XjXyFwt","executionInfo":{"status":"ok","timestamp":1658072065930,"user_tz":-540,"elapsed":336,"user":{"displayName":"Drive Google","userId":"18351879974904137523"}}},"execution_count":22,"outputs":[]},{"cell_type":"code","source":["# 모델 학습을 위한 함수를 정의하기\n","# 1 epoch를 훈련하는 함수이다\n","def train(model, iterator, optimizer, criterion, device):\n","  \n","  epoch_loss = 0\n","  epoch_acc = 0\n","  \n","  model.train()\n","  # train_dataloader로부터 한번에 배치 개수만큼 데이터를 불러오는 루틴이다. \n","  for (x, y) in iterator:\n","    x, y = x.to(device), y.to(device)\n","\n","    optimizer.zero_grad()\n","    y_pred, _ = model(x) # 아까 모델에서 2개의 값을 반환한다고 정의하였었다. 각 클래스별 확률값은 첫 번째 반환값으로 설정하였다.\n","    loss = criterion(y_pred, y)\n","    acc = calculate_accuracy(y_pred, y)\n","    loss.backward()\n","    optimizer.step()\n","\n","    # 평균 정확도와 평균 손실함수값을 축적해나간다\n","    epoch_loss += loss.item()\n","    epoch_acc += acc.item()\n","\n","  return epoch_loss / len(iterator), epoch_acc / len(iterator)"],"metadata":{"id":"kiDQlIZ6y-st","executionInfo":{"status":"ok","timestamp":1658072438205,"user_tz":-540,"elapsed":364,"user":{"displayName":"Drive Google","userId":"18351879974904137523"}}},"execution_count":24,"outputs":[]},{"cell_type":"code","source":["# 모델의 성능을 측정하는 함수\n","def evaluate(model, iterator, criterion, device):\n","  epoch_loss = 0\n","  epoch_acc = 0\n","\n","  model.eval()\n","  with torch.no_grad():\n","    for (x, y) in iterator:\n","      x, y = x.to(device), y.to(device)\n","      y_pred, _ = model(x)\n","      loss = criterion(y_pred, y)\n","      acc = calculate_accuracy(y_pred, y)\n","      epoch_loss += loss.item()\n","      epoch_acc += acc.item()\n","\n","  return epoch_loss / len(iterator), epoch_acc / len(iterator)"],"metadata":{"id":"VnEv8RnE0Zje","executionInfo":{"status":"ok","timestamp":1658072628684,"user_tz":-540,"elapsed":312,"user":{"displayName":"Drive Google","userId":"18351879974904137523"}}},"execution_count":25,"outputs":[]},{"cell_type":"code","source":["# 모델의 학습시간을 측정하기 위한 함수\n","def epoch_time(start_time, end_time):\n","  elapsed_time = end_time - start_time\n","  elapsed_mins = int(elapsed_time / 60)\n","  elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n","  return elapsed_mins, elapsed_secs"],"metadata":{"id":"LjVilhnI1KUK","executionInfo":{"status":"ok","timestamp":1658072706679,"user_tz":-540,"elapsed":285,"user":{"displayName":"Drive Google","userId":"18351879974904137523"}}},"execution_count":26,"outputs":[]},{"cell_type":"code","source":["# 모델을 학습\n","EPOCHS = 5\n","best_valid_loss = float('inf')\n","for epoch in range(EPOCHS):\n","  start_time = time.monotonic() # 현재 OS에서의 시간값 반환\n","  train_loss, train_acc = train(model, train_iterator, optimizer, criterion, device)\n","  valid_loss, valid_acc = evaluate(model, valid_iterator, criterion, device)\n","\n","  if valid_loss < best_valid_loss:\n","    best_valid_loss = valid_loss\n","    torch.save(model.state_dict(),\"./best_VGG_model.pt\")\n","\n","  end_time = time.monotonic()\n","  epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n","\n","  print(f\"EPOCH : {epoch+1} | Epoch Time : {epoch_mins}m {epoch_secs}s\")\n","  print(f\"Train Loss : {train_loss} | Train Acc : {train_acc}\")\n","  print(f\"Valid Loss : {valid_loss} | Valid Acc : {valid_acc}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":266},"id":"ZWVzBrfI1rA_","executionInfo":{"status":"error","timestamp":1658073041166,"user_tz":-540,"elapsed":296,"user":{"displayName":"Drive Google","userId":"18351879974904137523"}},"outputId":"a5a970bf-117e-4a81-aed8-6f38ec3ca312"},"execution_count":27,"outputs":[{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-27-7901a3873e86>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mbest_valid_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'inf'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEPOCHS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m   \u001b[0mstart_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmonotonic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# 현재 OS에서의 시간값 반환\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m   \u001b[0mtrain_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_iterator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m   \u001b[0mvalid_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_iterator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'time' is not defined"]}]},{"cell_type":"code","source":["# 앞서 저장했던 손실함수값이 가장 낮았던 모델을 불러와서 Test dataset으로 성능을 측정해보자\n","model.load_state_dict(\"./best_VGG_model.pt\")\n","test_loss, test_acc = evaluate(model, test_iterator, criterion, device)\n","print(f\"Test Loss : {test_loss} | Test Acc : {test_acc}\")\n","\n","\n","# Test데이터셋을 이용한 모델 성능 측정 결과\n","# Test loss는 0.684\n","# Test Acc는 0.5로 결과가 좋지 않다.\n","# 수많은 가중치가 있는데 반해 데이터가 너무 적기 때문이다.\n","# 데이터셋을 충분히 늘려야 성능이 좋아질것이다."],"metadata":{"id":"6e6aZjv12tc4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 테스트 데이터셋을 이용하여 모델의 예측 결과를 알아보기 위한 함수를 정의한다\n","# test_dataset의 모든 데이터에 대해 모델이 예측한 이미지 원본, 정답 label값, 확률값을 반환한다.\n","def get_predictions(model, iterator):\n","  model.eval()\n","  images = []\n","  labels = []\n","  probs = []\n","  with torch.no_grad():\n","    for (x, y) in iterator:\n","      x, y = x.to(device), y.to(device)\n","      y_pred, _ = model(x)\n","      y_prob = F.softmax(y_pred, dim=1)\n","      top_pred = y_prob.argmax(1, keepdim = True)\n","\n","      # image, label, prob모두 matplotlib에서 사용될 것이기 때문에 cpu로 옮겨놓는다\n","      images.append(x.cpu())\n","      labels.append(y.cpu()) \n","      probs.append(y_prob.cpu())\n","\n","\n","  images = torch.cat(images, dim=0)\n","  labels = torch.cat(labels, dim=0)\n","  probs = torch.cat(probs, dim=0)\n","\n","  return images, labels, probs"],"metadata":{"id":"qG3Je98w5-Hg","executionInfo":{"status":"ok","timestamp":1658074167430,"user_tz":-540,"elapsed":9,"user":{"displayName":"Drive Google","userId":"18351879974904137523"}}},"execution_count":29,"outputs":[]},{"cell_type":"code","source":["images, labels, probs = get_predictions(model, test_iterator)\n","# 차례대로 이미지 원본, 해당 이미지의 정답 레이블, 모델이 예측한 확률값이다.\n","pred_labels = torch.argmax(probs, 1)\n","corrects = torch.eq(labels, pred_labels)\n","correct_examples = []\n","\n","# 모델이 정답을 맞춘 이미지를 저장한다\n","for image, label, prob, correct in zip(images, labels, probs, corrects):\n","  if correct:\n","    correct_examples.append((image, label, prob))\n","\n","correct_example.sort(reverse=True, key=lambda x:torch.max(x[2], dim=0).values)"],"metadata":{"id":"FkyaJZC87BWw"},"execution_count":null,"outputs":[]}]}